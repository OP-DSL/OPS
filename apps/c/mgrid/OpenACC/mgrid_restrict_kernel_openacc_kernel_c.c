//
// auto-generated by ops.py
//

#define OPS_GPU

int xdim0_mgrid_restrict_kernel;
int xdim1_mgrid_restrict_kernel;


#undef OPS_ACC0
#undef OPS_ACC1


#define OPS_ACC0(x,y) (x+xdim0_mgrid_restrict_kernel*(y))
#define OPS_ACC1(x,y) (x+xdim1_mgrid_restrict_kernel*(y))

//user function
inline 
void mgrid_restrict_kernel(const double *fine, double *coarse, int *idx) {

  coarse[OPS_ACC1(0,0)] = fine[OPS_ACC0(0,0)];
}


#undef OPS_ACC0
#undef OPS_ACC1



void mgrid_restrict_kernel_c_wrapper(
  double *p_a0,
  int *stride_0,
  double *p_a1,
  int *p_a2,
  int arg_idx0, int arg_idx1,
  int global_idx0, int global_idx1,
  int x_size, int y_size) {
  int stride_00 = stride_0[0];
  int stride_01 = stride_0[1];
  #ifdef OPS_GPU
  #pragma acc parallel deviceptr(p_a0,p_a1)
  #pragma acc loop
  #endif
  for ( int n_y=0; n_y<y_size; n_y++ ){
    #ifdef OPS_GPU
    #pragma acc loop
    #endif
    for ( int n_x=0; n_x<x_size; n_x++ ){
      int arg_idx[] = {arg_idx0+n_x, arg_idx1+n_y};
      mgrid_restrict_kernel(  p_a0 + n_x*stride_00*1*1 + n_y*stride_01*xdim0_mgrid_restrict_kernel*1*1,
           p_a1 + n_x*1*1 + n_y*xdim1_mgrid_restrict_kernel*1*1,arg_idx );

    }
  }
}
